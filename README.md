# DocMind-AI-Multi-Modal-RAG-system-


# ğŸ§  DocMind â€“ Multi-Modal Retrieval-Augmented Generation System

> An AI system that reads, sees, understands, and answers from complex PDFs including text, tables, and images.

---

## ğŸš€ What is DocMind?

**DocMind** is a **Multi-Modal RAG (Retrieval-Augmented Generation)** pipeline that allows users to chat with PDFs containing:

- ğŸ“„ Text  
- ğŸ“Š Tables  
- ğŸ–¼ Images (diagrams, charts, figures)  

It uses **Gemini Vision models + vector search** to produce **accurate, context-aware answers** â€” even when information is stored in tables or images.

This system goes far beyond standard text-based RAG.

---

## ğŸ”¥ Why DocMind is Different

| Traditional RAG | DocMind |
|-----------------|-----------|
| Only text chunks | Text + tables + images |
| Loses visual data | Understands diagrams & charts |
| Weak on PDFs | Designed for PDFs |
| Hallucination prone | AI-generated searchable summaries |

DocMind builds **AI-enhanced embeddings** by understanding every modality before indexing.

---

## ğŸ§© System Architecture

PDF
â””â”€â”€ Unstructured Loader
â”œâ”€â”€ Text
â”œâ”€â”€ Tables
â””â”€â”€ Images
â†“
Gemini Vision + LLM
â†“
AI-Enhanced Chunk Summaries
â†“
Vector Embeddings
â†“
Chroma Vector Store
â†“
Conversational RAG Chain
â†“
Chat with Memory




---

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|--------|------------|
| LLM | Google Gemini 2.5 Flash |
| Vision | Gemini Vision |
| PDF Parsing | Unstructured |
| Chunking | Title-based chunking |
| Embeddings | Gemini Embeddings |
| Vector DB | Chroma |
| RAG | LangChain |
| Memory | ConversationBufferMemory |

---

## ğŸ§  What Happens Inside

### Step 1 â€“ PDF Ingestion
The PDF is parsed using **Unstructured** to extract:
- Text blocks  
- Tables (HTML)  
- Images (Base64)

---

### Step 2 â€“ Intelligent Chunking
Content is chunked by:
- Titles  
- Semantic boundaries  
ensuring logical sections stay together.

---

### Step 3 â€“ AI-Enhanced Summaries
Each chunk is passed through **Gemini Vision** to generate:

- Searchable descriptions  
- Extracted facts  
- Visual understanding of charts & diagrams  
- Alternative user search phrases  

This dramatically improves retrieval accuracy.

---

### Step 4 â€“ Vector Storage
Chunks are embedded using **Gemini Embeddings** and stored in **Chroma DB**.

---

### Step 5 â€“ Conversational RAG
A LangChain **ConversationalRetrievalChain** enables:

- Multi-turn chat  
- Memory-based follow-ups  
- Source-aware answers  

---

## ğŸ’¬ Example Queries

```text
Q1: What is the per-layer complexity of self-attention?
Q2: And why is it important?
